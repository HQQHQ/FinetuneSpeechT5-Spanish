{
  "best_metric": 0.4505269229412079,
  "best_model_checkpoint": "speecht5_finetuned_voxpopuli_nl/checkpoint-2000",
  "epoch": 8.64397622906537,
  "eval_steps": 1000,
  "global_step": 2000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.10804970286331712,
      "grad_norm": 4.890783309936523,
      "learning_rate": 5.000000000000001e-07,
      "loss": 0.7958,
      "step": 25
    },
    {
      "epoch": 0.21609940572663425,
      "grad_norm": 2.898012638092041,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 0.7689,
      "step": 50
    },
    {
      "epoch": 0.3241491085899514,
      "grad_norm": 3.471942901611328,
      "learning_rate": 1.48e-06,
      "loss": 0.7529,
      "step": 75
    },
    {
      "epoch": 0.4321988114532685,
      "grad_norm": 2.7499890327453613,
      "learning_rate": 1.98e-06,
      "loss": 0.7401,
      "step": 100
    },
    {
      "epoch": 0.5402485143165856,
      "grad_norm": 2.962912082672119,
      "learning_rate": 2.4800000000000004e-06,
      "loss": 0.7048,
      "step": 125
    },
    {
      "epoch": 0.6482982171799028,
      "grad_norm": 2.0028252601623535,
      "learning_rate": 2.9800000000000003e-06,
      "loss": 0.6826,
      "step": 150
    },
    {
      "epoch": 0.7563479200432199,
      "grad_norm": 2.515033721923828,
      "learning_rate": 3.48e-06,
      "loss": 0.6655,
      "step": 175
    },
    {
      "epoch": 0.864397622906537,
      "grad_norm": 2.316354751586914,
      "learning_rate": 3.980000000000001e-06,
      "loss": 0.6526,
      "step": 200
    },
    {
      "epoch": 0.9724473257698542,
      "grad_norm": 1.84903883934021,
      "learning_rate": 4.48e-06,
      "loss": 0.6532,
      "step": 225
    },
    {
      "epoch": 1.0804970286331712,
      "grad_norm": 1.8633564710617065,
      "learning_rate": 4.980000000000001e-06,
      "loss": 0.6393,
      "step": 250
    },
    {
      "epoch": 1.1885467314964884,
      "grad_norm": 1.7015737295150757,
      "learning_rate": 5.480000000000001e-06,
      "loss": 0.6304,
      "step": 275
    },
    {
      "epoch": 1.2965964343598055,
      "grad_norm": 2.144501209259033,
      "learning_rate": 5.98e-06,
      "loss": 0.6049,
      "step": 300
    },
    {
      "epoch": 1.4046461372231227,
      "grad_norm": 2.430676221847534,
      "learning_rate": 6.480000000000001e-06,
      "loss": 0.5994,
      "step": 325
    },
    {
      "epoch": 1.5126958400864399,
      "grad_norm": 1.6690797805786133,
      "learning_rate": 6.98e-06,
      "loss": 0.5709,
      "step": 350
    },
    {
      "epoch": 1.620745542949757,
      "grad_norm": 1.7207589149475098,
      "learning_rate": 7.48e-06,
      "loss": 0.5645,
      "step": 375
    },
    {
      "epoch": 1.728795245813074,
      "grad_norm": 2.163778781890869,
      "learning_rate": 7.980000000000002e-06,
      "loss": 0.5607,
      "step": 400
    },
    {
      "epoch": 1.8368449486763911,
      "grad_norm": 1.4455478191375732,
      "learning_rate": 8.48e-06,
      "loss": 0.5574,
      "step": 425
    },
    {
      "epoch": 1.9448946515397083,
      "grad_norm": 1.6066086292266846,
      "learning_rate": 8.98e-06,
      "loss": 0.5436,
      "step": 450
    },
    {
      "epoch": 2.0529443544030253,
      "grad_norm": 1.5660029649734497,
      "learning_rate": 9.48e-06,
      "loss": 0.5466,
      "step": 475
    },
    {
      "epoch": 2.1609940572663424,
      "grad_norm": 1.3218941688537598,
      "learning_rate": 9.980000000000001e-06,
      "loss": 0.5382,
      "step": 500
    },
    {
      "epoch": 2.2690437601296596,
      "grad_norm": 1.5174471139907837,
      "learning_rate": 9.931428571428571e-06,
      "loss": 0.5419,
      "step": 525
    },
    {
      "epoch": 2.3770934629929767,
      "grad_norm": 1.1267651319503784,
      "learning_rate": 9.86e-06,
      "loss": 0.5254,
      "step": 550
    },
    {
      "epoch": 2.485143165856294,
      "grad_norm": 1.5505274534225464,
      "learning_rate": 9.78857142857143e-06,
      "loss": 0.5316,
      "step": 575
    },
    {
      "epoch": 2.593192868719611,
      "grad_norm": 1.5775372982025146,
      "learning_rate": 9.717142857142858e-06,
      "loss": 0.5252,
      "step": 600
    },
    {
      "epoch": 2.7012425715829282,
      "grad_norm": 1.435638189315796,
      "learning_rate": 9.645714285714286e-06,
      "loss": 0.5261,
      "step": 625
    },
    {
      "epoch": 2.8092922744462454,
      "grad_norm": 1.428922414779663,
      "learning_rate": 9.574285714285715e-06,
      "loss": 0.5276,
      "step": 650
    },
    {
      "epoch": 2.9173419773095626,
      "grad_norm": 1.7638425827026367,
      "learning_rate": 9.502857142857144e-06,
      "loss": 0.524,
      "step": 675
    },
    {
      "epoch": 3.0253916801728797,
      "grad_norm": 1.120676875114441,
      "learning_rate": 9.431428571428573e-06,
      "loss": 0.5258,
      "step": 700
    },
    {
      "epoch": 3.1334413830361965,
      "grad_norm": 1.4151748418807983,
      "learning_rate": 9.360000000000002e-06,
      "loss": 0.515,
      "step": 725
    },
    {
      "epoch": 3.2414910858995136,
      "grad_norm": 1.3693028688430786,
      "learning_rate": 9.28857142857143e-06,
      "loss": 0.517,
      "step": 750
    },
    {
      "epoch": 3.349540788762831,
      "grad_norm": 1.541468620300293,
      "learning_rate": 9.217142857142858e-06,
      "loss": 0.5197,
      "step": 775
    },
    {
      "epoch": 3.457590491626148,
      "grad_norm": 1.2785146236419678,
      "learning_rate": 9.145714285714287e-06,
      "loss": 0.5117,
      "step": 800
    },
    {
      "epoch": 3.565640194489465,
      "grad_norm": 1.4224395751953125,
      "learning_rate": 9.074285714285716e-06,
      "loss": 0.5104,
      "step": 825
    },
    {
      "epoch": 3.6736898973527823,
      "grad_norm": 1.2443407773971558,
      "learning_rate": 9.002857142857144e-06,
      "loss": 0.5087,
      "step": 850
    },
    {
      "epoch": 3.7817396002160995,
      "grad_norm": 1.39913010597229,
      "learning_rate": 8.931428571428573e-06,
      "loss": 0.5118,
      "step": 875
    },
    {
      "epoch": 3.8897893030794166,
      "grad_norm": 1.1192206144332886,
      "learning_rate": 8.860000000000002e-06,
      "loss": 0.5069,
      "step": 900
    },
    {
      "epoch": 3.997839005942734,
      "grad_norm": 1.358802318572998,
      "learning_rate": 8.788571428571429e-06,
      "loss": 0.5118,
      "step": 925
    },
    {
      "epoch": 4.1058887088060505,
      "grad_norm": 1.4780887365341187,
      "learning_rate": 8.717142857142858e-06,
      "loss": 0.503,
      "step": 950
    },
    {
      "epoch": 4.213938411669368,
      "grad_norm": 1.0592700242996216,
      "learning_rate": 8.645714285714287e-06,
      "loss": 0.5066,
      "step": 975
    },
    {
      "epoch": 4.321988114532685,
      "grad_norm": 1.2728278636932373,
      "learning_rate": 8.574285714285714e-06,
      "loss": 0.506,
      "step": 1000
    },
    {
      "epoch": 4.321988114532685,
      "eval_loss": 0.46402496099472046,
      "eval_runtime": 31.9452,
      "eval_samples_per_second": 25.763,
      "eval_steps_per_second": 12.897,
      "step": 1000
    },
    {
      "epoch": 4.430037817396002,
      "grad_norm": 1.1157643795013428,
      "learning_rate": 8.502857142857143e-06,
      "loss": 0.5003,
      "step": 1025
    },
    {
      "epoch": 4.538087520259319,
      "grad_norm": 1.1354410648345947,
      "learning_rate": 8.431428571428572e-06,
      "loss": 0.5053,
      "step": 1050
    },
    {
      "epoch": 4.646137223122636,
      "grad_norm": 1.2353136539459229,
      "learning_rate": 8.36e-06,
      "loss": 0.504,
      "step": 1075
    },
    {
      "epoch": 4.7541869259859535,
      "grad_norm": 1.143047571182251,
      "learning_rate": 8.288571428571429e-06,
      "loss": 0.5028,
      "step": 1100
    },
    {
      "epoch": 4.862236628849271,
      "grad_norm": 1.2388845682144165,
      "learning_rate": 8.217142857142858e-06,
      "loss": 0.4996,
      "step": 1125
    },
    {
      "epoch": 4.970286331712588,
      "grad_norm": 1.37420654296875,
      "learning_rate": 8.145714285714287e-06,
      "loss": 0.5005,
      "step": 1150
    },
    {
      "epoch": 5.078336034575905,
      "grad_norm": 1.058274507522583,
      "learning_rate": 8.074285714285714e-06,
      "loss": 0.5013,
      "step": 1175
    },
    {
      "epoch": 5.186385737439222,
      "grad_norm": 1.2124207019805908,
      "learning_rate": 8.002857142857143e-06,
      "loss": 0.4941,
      "step": 1200
    },
    {
      "epoch": 5.294435440302539,
      "grad_norm": 1.3366544246673584,
      "learning_rate": 7.931428571428572e-06,
      "loss": 0.4967,
      "step": 1225
    },
    {
      "epoch": 5.4024851431658565,
      "grad_norm": 1.180616855621338,
      "learning_rate": 7.860000000000001e-06,
      "loss": 0.4972,
      "step": 1250
    },
    {
      "epoch": 5.510534846029174,
      "grad_norm": 1.6939785480499268,
      "learning_rate": 7.788571428571428e-06,
      "loss": 0.4989,
      "step": 1275
    },
    {
      "epoch": 5.618584548892491,
      "grad_norm": 1.0262173414230347,
      "learning_rate": 7.717142857142857e-06,
      "loss": 0.4952,
      "step": 1300
    },
    {
      "epoch": 5.726634251755808,
      "grad_norm": 1.2322193384170532,
      "learning_rate": 7.645714285714286e-06,
      "loss": 0.4975,
      "step": 1325
    },
    {
      "epoch": 5.834683954619125,
      "grad_norm": 1.5651936531066895,
      "learning_rate": 7.574285714285715e-06,
      "loss": 0.4988,
      "step": 1350
    },
    {
      "epoch": 5.942733657482442,
      "grad_norm": 1.322357177734375,
      "learning_rate": 7.502857142857144e-06,
      "loss": 0.5001,
      "step": 1375
    },
    {
      "epoch": 6.0507833603457595,
      "grad_norm": 1.106981873512268,
      "learning_rate": 7.431428571428572e-06,
      "loss": 0.4953,
      "step": 1400
    },
    {
      "epoch": 6.158833063209076,
      "grad_norm": 1.0792356729507446,
      "learning_rate": 7.360000000000001e-06,
      "loss": 0.4976,
      "step": 1425
    },
    {
      "epoch": 6.266882766072393,
      "grad_norm": 1.5821036100387573,
      "learning_rate": 7.28857142857143e-06,
      "loss": 0.5012,
      "step": 1450
    },
    {
      "epoch": 6.37493246893571,
      "grad_norm": 1.520245909690857,
      "learning_rate": 7.217142857142858e-06,
      "loss": 0.4915,
      "step": 1475
    },
    {
      "epoch": 6.482982171799027,
      "grad_norm": 1.5036336183547974,
      "learning_rate": 7.145714285714286e-06,
      "loss": 0.4986,
      "step": 1500
    },
    {
      "epoch": 6.591031874662344,
      "grad_norm": 1.5799757242202759,
      "learning_rate": 7.074285714285715e-06,
      "loss": 0.4904,
      "step": 1525
    },
    {
      "epoch": 6.699081577525662,
      "grad_norm": 1.4973798990249634,
      "learning_rate": 7.002857142857143e-06,
      "loss": 0.4927,
      "step": 1550
    },
    {
      "epoch": 6.807131280388979,
      "grad_norm": 1.2324672937393188,
      "learning_rate": 6.931428571428572e-06,
      "loss": 0.4909,
      "step": 1575
    },
    {
      "epoch": 6.915180983252296,
      "grad_norm": 1.3564220666885376,
      "learning_rate": 6.860000000000001e-06,
      "loss": 0.4965,
      "step": 1600
    },
    {
      "epoch": 7.023230686115613,
      "grad_norm": 1.169512152671814,
      "learning_rate": 6.7885714285714286e-06,
      "loss": 0.4898,
      "step": 1625
    },
    {
      "epoch": 7.13128038897893,
      "grad_norm": 1.7638176679611206,
      "learning_rate": 6.7171428571428576e-06,
      "loss": 0.4883,
      "step": 1650
    },
    {
      "epoch": 7.239330091842247,
      "grad_norm": 1.7857189178466797,
      "learning_rate": 6.645714285714287e-06,
      "loss": 0.4908,
      "step": 1675
    },
    {
      "epoch": 7.347379794705565,
      "grad_norm": 1.1459699869155884,
      "learning_rate": 6.574285714285716e-06,
      "loss": 0.4935,
      "step": 1700
    },
    {
      "epoch": 7.455429497568882,
      "grad_norm": 1.7065739631652832,
      "learning_rate": 6.502857142857143e-06,
      "loss": 0.4928,
      "step": 1725
    },
    {
      "epoch": 7.563479200432199,
      "grad_norm": 1.0383408069610596,
      "learning_rate": 6.431428571428572e-06,
      "loss": 0.4907,
      "step": 1750
    },
    {
      "epoch": 7.671528903295516,
      "grad_norm": 1.2544556856155396,
      "learning_rate": 6.360000000000001e-06,
      "loss": 0.4939,
      "step": 1775
    },
    {
      "epoch": 7.779578606158833,
      "grad_norm": 1.0397534370422363,
      "learning_rate": 6.288571428571429e-06,
      "loss": 0.4937,
      "step": 1800
    },
    {
      "epoch": 7.88762830902215,
      "grad_norm": 1.1375797986984253,
      "learning_rate": 6.217142857142857e-06,
      "loss": 0.49,
      "step": 1825
    },
    {
      "epoch": 7.995678011885468,
      "grad_norm": 1.0957820415496826,
      "learning_rate": 6.145714285714286e-06,
      "loss": 0.4888,
      "step": 1850
    },
    {
      "epoch": 8.103727714748784,
      "grad_norm": 1.1376097202301025,
      "learning_rate": 6.0742857142857145e-06,
      "loss": 0.4873,
      "step": 1875
    },
    {
      "epoch": 8.211777417612101,
      "grad_norm": 1.3572498559951782,
      "learning_rate": 6.0028571428571435e-06,
      "loss": 0.4886,
      "step": 1900
    },
    {
      "epoch": 8.319827120475418,
      "grad_norm": 1.5002219676971436,
      "learning_rate": 5.9314285714285725e-06,
      "loss": 0.4855,
      "step": 1925
    },
    {
      "epoch": 8.427876823338735,
      "grad_norm": 1.091207504272461,
      "learning_rate": 5.86e-06,
      "loss": 0.486,
      "step": 1950
    },
    {
      "epoch": 8.535926526202053,
      "grad_norm": 1.4525457620620728,
      "learning_rate": 5.788571428571429e-06,
      "loss": 0.4896,
      "step": 1975
    },
    {
      "epoch": 8.64397622906537,
      "grad_norm": 1.1560490131378174,
      "learning_rate": 5.717142857142858e-06,
      "loss": 0.4854,
      "step": 2000
    },
    {
      "epoch": 8.64397622906537,
      "eval_loss": 0.4505269229412079,
      "eval_runtime": 30.4977,
      "eval_samples_per_second": 26.986,
      "eval_steps_per_second": 13.509,
      "step": 2000
    }
  ],
  "logging_steps": 25,
  "max_steps": 4000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 18,
  "save_steps": 1000,
  "total_flos": 9194723870536512.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
