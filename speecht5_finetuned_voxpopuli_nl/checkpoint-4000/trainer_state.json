{
  "best_metric": 0.4458176791667938,
  "best_model_checkpoint": "speecht5_finetuned_voxpopuli_nl/checkpoint-4000",
  "epoch": 17.28795245813074,
  "eval_steps": 1000,
  "global_step": 4000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.10804970286331712,
      "grad_norm": 4.890783309936523,
      "learning_rate": 5.000000000000001e-07,
      "loss": 0.7958,
      "step": 25
    },
    {
      "epoch": 0.21609940572663425,
      "grad_norm": 2.898012638092041,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 0.7689,
      "step": 50
    },
    {
      "epoch": 0.3241491085899514,
      "grad_norm": 3.471942901611328,
      "learning_rate": 1.48e-06,
      "loss": 0.7529,
      "step": 75
    },
    {
      "epoch": 0.4321988114532685,
      "grad_norm": 2.7499890327453613,
      "learning_rate": 1.98e-06,
      "loss": 0.7401,
      "step": 100
    },
    {
      "epoch": 0.5402485143165856,
      "grad_norm": 2.962912082672119,
      "learning_rate": 2.4800000000000004e-06,
      "loss": 0.7048,
      "step": 125
    },
    {
      "epoch": 0.6482982171799028,
      "grad_norm": 2.0028252601623535,
      "learning_rate": 2.9800000000000003e-06,
      "loss": 0.6826,
      "step": 150
    },
    {
      "epoch": 0.7563479200432199,
      "grad_norm": 2.515033721923828,
      "learning_rate": 3.48e-06,
      "loss": 0.6655,
      "step": 175
    },
    {
      "epoch": 0.864397622906537,
      "grad_norm": 2.316354751586914,
      "learning_rate": 3.980000000000001e-06,
      "loss": 0.6526,
      "step": 200
    },
    {
      "epoch": 0.9724473257698542,
      "grad_norm": 1.84903883934021,
      "learning_rate": 4.48e-06,
      "loss": 0.6532,
      "step": 225
    },
    {
      "epoch": 1.0804970286331712,
      "grad_norm": 1.8633564710617065,
      "learning_rate": 4.980000000000001e-06,
      "loss": 0.6393,
      "step": 250
    },
    {
      "epoch": 1.1885467314964884,
      "grad_norm": 1.7015737295150757,
      "learning_rate": 5.480000000000001e-06,
      "loss": 0.6304,
      "step": 275
    },
    {
      "epoch": 1.2965964343598055,
      "grad_norm": 2.144501209259033,
      "learning_rate": 5.98e-06,
      "loss": 0.6049,
      "step": 300
    },
    {
      "epoch": 1.4046461372231227,
      "grad_norm": 2.430676221847534,
      "learning_rate": 6.480000000000001e-06,
      "loss": 0.5994,
      "step": 325
    },
    {
      "epoch": 1.5126958400864399,
      "grad_norm": 1.6690797805786133,
      "learning_rate": 6.98e-06,
      "loss": 0.5709,
      "step": 350
    },
    {
      "epoch": 1.620745542949757,
      "grad_norm": 1.7207589149475098,
      "learning_rate": 7.48e-06,
      "loss": 0.5645,
      "step": 375
    },
    {
      "epoch": 1.728795245813074,
      "grad_norm": 2.163778781890869,
      "learning_rate": 7.980000000000002e-06,
      "loss": 0.5607,
      "step": 400
    },
    {
      "epoch": 1.8368449486763911,
      "grad_norm": 1.4455478191375732,
      "learning_rate": 8.48e-06,
      "loss": 0.5574,
      "step": 425
    },
    {
      "epoch": 1.9448946515397083,
      "grad_norm": 1.6066086292266846,
      "learning_rate": 8.98e-06,
      "loss": 0.5436,
      "step": 450
    },
    {
      "epoch": 2.0529443544030253,
      "grad_norm": 1.5660029649734497,
      "learning_rate": 9.48e-06,
      "loss": 0.5466,
      "step": 475
    },
    {
      "epoch": 2.1609940572663424,
      "grad_norm": 1.3218941688537598,
      "learning_rate": 9.980000000000001e-06,
      "loss": 0.5382,
      "step": 500
    },
    {
      "epoch": 2.2690437601296596,
      "grad_norm": 1.5174471139907837,
      "learning_rate": 9.931428571428571e-06,
      "loss": 0.5419,
      "step": 525
    },
    {
      "epoch": 2.3770934629929767,
      "grad_norm": 1.1267651319503784,
      "learning_rate": 9.86e-06,
      "loss": 0.5254,
      "step": 550
    },
    {
      "epoch": 2.485143165856294,
      "grad_norm": 1.5505274534225464,
      "learning_rate": 9.78857142857143e-06,
      "loss": 0.5316,
      "step": 575
    },
    {
      "epoch": 2.593192868719611,
      "grad_norm": 1.5775372982025146,
      "learning_rate": 9.717142857142858e-06,
      "loss": 0.5252,
      "step": 600
    },
    {
      "epoch": 2.7012425715829282,
      "grad_norm": 1.435638189315796,
      "learning_rate": 9.645714285714286e-06,
      "loss": 0.5261,
      "step": 625
    },
    {
      "epoch": 2.8092922744462454,
      "grad_norm": 1.428922414779663,
      "learning_rate": 9.574285714285715e-06,
      "loss": 0.5276,
      "step": 650
    },
    {
      "epoch": 2.9173419773095626,
      "grad_norm": 1.7638425827026367,
      "learning_rate": 9.502857142857144e-06,
      "loss": 0.524,
      "step": 675
    },
    {
      "epoch": 3.0253916801728797,
      "grad_norm": 1.120676875114441,
      "learning_rate": 9.431428571428573e-06,
      "loss": 0.5258,
      "step": 700
    },
    {
      "epoch": 3.1334413830361965,
      "grad_norm": 1.4151748418807983,
      "learning_rate": 9.360000000000002e-06,
      "loss": 0.515,
      "step": 725
    },
    {
      "epoch": 3.2414910858995136,
      "grad_norm": 1.3693028688430786,
      "learning_rate": 9.28857142857143e-06,
      "loss": 0.517,
      "step": 750
    },
    {
      "epoch": 3.349540788762831,
      "grad_norm": 1.541468620300293,
      "learning_rate": 9.217142857142858e-06,
      "loss": 0.5197,
      "step": 775
    },
    {
      "epoch": 3.457590491626148,
      "grad_norm": 1.2785146236419678,
      "learning_rate": 9.145714285714287e-06,
      "loss": 0.5117,
      "step": 800
    },
    {
      "epoch": 3.565640194489465,
      "grad_norm": 1.4224395751953125,
      "learning_rate": 9.074285714285716e-06,
      "loss": 0.5104,
      "step": 825
    },
    {
      "epoch": 3.6736898973527823,
      "grad_norm": 1.2443407773971558,
      "learning_rate": 9.002857142857144e-06,
      "loss": 0.5087,
      "step": 850
    },
    {
      "epoch": 3.7817396002160995,
      "grad_norm": 1.39913010597229,
      "learning_rate": 8.931428571428573e-06,
      "loss": 0.5118,
      "step": 875
    },
    {
      "epoch": 3.8897893030794166,
      "grad_norm": 1.1192206144332886,
      "learning_rate": 8.860000000000002e-06,
      "loss": 0.5069,
      "step": 900
    },
    {
      "epoch": 3.997839005942734,
      "grad_norm": 1.358802318572998,
      "learning_rate": 8.788571428571429e-06,
      "loss": 0.5118,
      "step": 925
    },
    {
      "epoch": 4.1058887088060505,
      "grad_norm": 1.4780887365341187,
      "learning_rate": 8.717142857142858e-06,
      "loss": 0.503,
      "step": 950
    },
    {
      "epoch": 4.213938411669368,
      "grad_norm": 1.0592700242996216,
      "learning_rate": 8.645714285714287e-06,
      "loss": 0.5066,
      "step": 975
    },
    {
      "epoch": 4.321988114532685,
      "grad_norm": 1.2728278636932373,
      "learning_rate": 8.574285714285714e-06,
      "loss": 0.506,
      "step": 1000
    },
    {
      "epoch": 4.321988114532685,
      "eval_loss": 0.46402496099472046,
      "eval_runtime": 31.9452,
      "eval_samples_per_second": 25.763,
      "eval_steps_per_second": 12.897,
      "step": 1000
    },
    {
      "epoch": 4.430037817396002,
      "grad_norm": 1.1157643795013428,
      "learning_rate": 8.502857142857143e-06,
      "loss": 0.5003,
      "step": 1025
    },
    {
      "epoch": 4.538087520259319,
      "grad_norm": 1.1354410648345947,
      "learning_rate": 8.431428571428572e-06,
      "loss": 0.5053,
      "step": 1050
    },
    {
      "epoch": 4.646137223122636,
      "grad_norm": 1.2353136539459229,
      "learning_rate": 8.36e-06,
      "loss": 0.504,
      "step": 1075
    },
    {
      "epoch": 4.7541869259859535,
      "grad_norm": 1.143047571182251,
      "learning_rate": 8.288571428571429e-06,
      "loss": 0.5028,
      "step": 1100
    },
    {
      "epoch": 4.862236628849271,
      "grad_norm": 1.2388845682144165,
      "learning_rate": 8.217142857142858e-06,
      "loss": 0.4996,
      "step": 1125
    },
    {
      "epoch": 4.970286331712588,
      "grad_norm": 1.37420654296875,
      "learning_rate": 8.145714285714287e-06,
      "loss": 0.5005,
      "step": 1150
    },
    {
      "epoch": 5.078336034575905,
      "grad_norm": 1.058274507522583,
      "learning_rate": 8.074285714285714e-06,
      "loss": 0.5013,
      "step": 1175
    },
    {
      "epoch": 5.186385737439222,
      "grad_norm": 1.2124207019805908,
      "learning_rate": 8.002857142857143e-06,
      "loss": 0.4941,
      "step": 1200
    },
    {
      "epoch": 5.294435440302539,
      "grad_norm": 1.3366544246673584,
      "learning_rate": 7.931428571428572e-06,
      "loss": 0.4967,
      "step": 1225
    },
    {
      "epoch": 5.4024851431658565,
      "grad_norm": 1.180616855621338,
      "learning_rate": 7.860000000000001e-06,
      "loss": 0.4972,
      "step": 1250
    },
    {
      "epoch": 5.510534846029174,
      "grad_norm": 1.6939785480499268,
      "learning_rate": 7.788571428571428e-06,
      "loss": 0.4989,
      "step": 1275
    },
    {
      "epoch": 5.618584548892491,
      "grad_norm": 1.0262173414230347,
      "learning_rate": 7.717142857142857e-06,
      "loss": 0.4952,
      "step": 1300
    },
    {
      "epoch": 5.726634251755808,
      "grad_norm": 1.2322193384170532,
      "learning_rate": 7.645714285714286e-06,
      "loss": 0.4975,
      "step": 1325
    },
    {
      "epoch": 5.834683954619125,
      "grad_norm": 1.5651936531066895,
      "learning_rate": 7.574285714285715e-06,
      "loss": 0.4988,
      "step": 1350
    },
    {
      "epoch": 5.942733657482442,
      "grad_norm": 1.322357177734375,
      "learning_rate": 7.502857142857144e-06,
      "loss": 0.5001,
      "step": 1375
    },
    {
      "epoch": 6.0507833603457595,
      "grad_norm": 1.106981873512268,
      "learning_rate": 7.431428571428572e-06,
      "loss": 0.4953,
      "step": 1400
    },
    {
      "epoch": 6.158833063209076,
      "grad_norm": 1.0792356729507446,
      "learning_rate": 7.360000000000001e-06,
      "loss": 0.4976,
      "step": 1425
    },
    {
      "epoch": 6.266882766072393,
      "grad_norm": 1.5821036100387573,
      "learning_rate": 7.28857142857143e-06,
      "loss": 0.5012,
      "step": 1450
    },
    {
      "epoch": 6.37493246893571,
      "grad_norm": 1.520245909690857,
      "learning_rate": 7.217142857142858e-06,
      "loss": 0.4915,
      "step": 1475
    },
    {
      "epoch": 6.482982171799027,
      "grad_norm": 1.5036336183547974,
      "learning_rate": 7.145714285714286e-06,
      "loss": 0.4986,
      "step": 1500
    },
    {
      "epoch": 6.591031874662344,
      "grad_norm": 1.5799757242202759,
      "learning_rate": 7.074285714285715e-06,
      "loss": 0.4904,
      "step": 1525
    },
    {
      "epoch": 6.699081577525662,
      "grad_norm": 1.4973798990249634,
      "learning_rate": 7.002857142857143e-06,
      "loss": 0.4927,
      "step": 1550
    },
    {
      "epoch": 6.807131280388979,
      "grad_norm": 1.2324672937393188,
      "learning_rate": 6.931428571428572e-06,
      "loss": 0.4909,
      "step": 1575
    },
    {
      "epoch": 6.915180983252296,
      "grad_norm": 1.3564220666885376,
      "learning_rate": 6.860000000000001e-06,
      "loss": 0.4965,
      "step": 1600
    },
    {
      "epoch": 7.023230686115613,
      "grad_norm": 1.169512152671814,
      "learning_rate": 6.7885714285714286e-06,
      "loss": 0.4898,
      "step": 1625
    },
    {
      "epoch": 7.13128038897893,
      "grad_norm": 1.7638176679611206,
      "learning_rate": 6.7171428571428576e-06,
      "loss": 0.4883,
      "step": 1650
    },
    {
      "epoch": 7.239330091842247,
      "grad_norm": 1.7857189178466797,
      "learning_rate": 6.645714285714287e-06,
      "loss": 0.4908,
      "step": 1675
    },
    {
      "epoch": 7.347379794705565,
      "grad_norm": 1.1459699869155884,
      "learning_rate": 6.574285714285716e-06,
      "loss": 0.4935,
      "step": 1700
    },
    {
      "epoch": 7.455429497568882,
      "grad_norm": 1.7065739631652832,
      "learning_rate": 6.502857142857143e-06,
      "loss": 0.4928,
      "step": 1725
    },
    {
      "epoch": 7.563479200432199,
      "grad_norm": 1.0383408069610596,
      "learning_rate": 6.431428571428572e-06,
      "loss": 0.4907,
      "step": 1750
    },
    {
      "epoch": 7.671528903295516,
      "grad_norm": 1.2544556856155396,
      "learning_rate": 6.360000000000001e-06,
      "loss": 0.4939,
      "step": 1775
    },
    {
      "epoch": 7.779578606158833,
      "grad_norm": 1.0397534370422363,
      "learning_rate": 6.288571428571429e-06,
      "loss": 0.4937,
      "step": 1800
    },
    {
      "epoch": 7.88762830902215,
      "grad_norm": 1.1375797986984253,
      "learning_rate": 6.217142857142857e-06,
      "loss": 0.49,
      "step": 1825
    },
    {
      "epoch": 7.995678011885468,
      "grad_norm": 1.0957820415496826,
      "learning_rate": 6.145714285714286e-06,
      "loss": 0.4888,
      "step": 1850
    },
    {
      "epoch": 8.103727714748784,
      "grad_norm": 1.1376097202301025,
      "learning_rate": 6.0742857142857145e-06,
      "loss": 0.4873,
      "step": 1875
    },
    {
      "epoch": 8.211777417612101,
      "grad_norm": 1.3572498559951782,
      "learning_rate": 6.0028571428571435e-06,
      "loss": 0.4886,
      "step": 1900
    },
    {
      "epoch": 8.319827120475418,
      "grad_norm": 1.5002219676971436,
      "learning_rate": 5.9314285714285725e-06,
      "loss": 0.4855,
      "step": 1925
    },
    {
      "epoch": 8.427876823338735,
      "grad_norm": 1.091207504272461,
      "learning_rate": 5.86e-06,
      "loss": 0.486,
      "step": 1950
    },
    {
      "epoch": 8.535926526202053,
      "grad_norm": 1.4525457620620728,
      "learning_rate": 5.788571428571429e-06,
      "loss": 0.4896,
      "step": 1975
    },
    {
      "epoch": 8.64397622906537,
      "grad_norm": 1.1560490131378174,
      "learning_rate": 5.717142857142858e-06,
      "loss": 0.4854,
      "step": 2000
    },
    {
      "epoch": 8.64397622906537,
      "eval_loss": 0.4505269229412079,
      "eval_runtime": 30.4977,
      "eval_samples_per_second": 26.986,
      "eval_steps_per_second": 13.509,
      "step": 2000
    },
    {
      "epoch": 8.752025931928687,
      "grad_norm": 1.1483938694000244,
      "learning_rate": 5.645714285714287e-06,
      "loss": 0.4899,
      "step": 2025
    },
    {
      "epoch": 8.860075634792004,
      "grad_norm": 1.3832998275756836,
      "learning_rate": 5.574285714285714e-06,
      "loss": 0.4839,
      "step": 2050
    },
    {
      "epoch": 8.968125337655321,
      "grad_norm": 1.1483659744262695,
      "learning_rate": 5.502857142857143e-06,
      "loss": 0.4832,
      "step": 2075
    },
    {
      "epoch": 9.076175040518638,
      "grad_norm": 1.2139641046524048,
      "learning_rate": 5.431428571428572e-06,
      "loss": 0.4869,
      "step": 2100
    },
    {
      "epoch": 9.184224743381955,
      "grad_norm": 1.252794623374939,
      "learning_rate": 5.36e-06,
      "loss": 0.4859,
      "step": 2125
    },
    {
      "epoch": 9.292274446245273,
      "grad_norm": 1.126525640487671,
      "learning_rate": 5.2885714285714285e-06,
      "loss": 0.4881,
      "step": 2150
    },
    {
      "epoch": 9.40032414910859,
      "grad_norm": 1.2820775508880615,
      "learning_rate": 5.2171428571428575e-06,
      "loss": 0.4852,
      "step": 2175
    },
    {
      "epoch": 9.508373851971907,
      "grad_norm": 1.2706143856048584,
      "learning_rate": 5.145714285714286e-06,
      "loss": 0.4853,
      "step": 2200
    },
    {
      "epoch": 9.616423554835224,
      "grad_norm": 1.203969955444336,
      "learning_rate": 5.074285714285715e-06,
      "loss": 0.4831,
      "step": 2225
    },
    {
      "epoch": 9.724473257698541,
      "grad_norm": 1.1171232461929321,
      "learning_rate": 5.002857142857144e-06,
      "loss": 0.4869,
      "step": 2250
    },
    {
      "epoch": 9.832522960561858,
      "grad_norm": 1.1408103704452515,
      "learning_rate": 4.931428571428572e-06,
      "loss": 0.4866,
      "step": 2275
    },
    {
      "epoch": 9.940572663425176,
      "grad_norm": 1.1363767385482788,
      "learning_rate": 4.86e-06,
      "loss": 0.4854,
      "step": 2300
    },
    {
      "epoch": 10.048622366288493,
      "grad_norm": 1.2251365184783936,
      "learning_rate": 4.788571428571429e-06,
      "loss": 0.4873,
      "step": 2325
    },
    {
      "epoch": 10.15667206915181,
      "grad_norm": 1.4957242012023926,
      "learning_rate": 4.717142857142857e-06,
      "loss": 0.4855,
      "step": 2350
    },
    {
      "epoch": 10.264721772015127,
      "grad_norm": 1.3086656332015991,
      "learning_rate": 4.645714285714286e-06,
      "loss": 0.4808,
      "step": 2375
    },
    {
      "epoch": 10.372771474878444,
      "grad_norm": 1.121464729309082,
      "learning_rate": 4.574285714285714e-06,
      "loss": 0.4919,
      "step": 2400
    },
    {
      "epoch": 10.480821177741761,
      "grad_norm": 1.5134271383285522,
      "learning_rate": 4.5028571428571434e-06,
      "loss": 0.4808,
      "step": 2425
    },
    {
      "epoch": 10.588870880605079,
      "grad_norm": 1.0745494365692139,
      "learning_rate": 4.431428571428572e-06,
      "loss": 0.4875,
      "step": 2450
    },
    {
      "epoch": 10.696920583468396,
      "grad_norm": 1.1386774778366089,
      "learning_rate": 4.360000000000001e-06,
      "loss": 0.4846,
      "step": 2475
    },
    {
      "epoch": 10.804970286331713,
      "grad_norm": 1.0529447793960571,
      "learning_rate": 4.288571428571429e-06,
      "loss": 0.4888,
      "step": 2500
    },
    {
      "epoch": 10.91301998919503,
      "grad_norm": 1.188435435295105,
      "learning_rate": 4.217142857142858e-06,
      "loss": 0.4843,
      "step": 2525
    },
    {
      "epoch": 11.021069692058347,
      "grad_norm": 1.3636188507080078,
      "learning_rate": 4.145714285714286e-06,
      "loss": 0.4799,
      "step": 2550
    },
    {
      "epoch": 11.129119394921664,
      "grad_norm": 1.0392638444900513,
      "learning_rate": 4.074285714285714e-06,
      "loss": 0.4824,
      "step": 2575
    },
    {
      "epoch": 11.237169097784982,
      "grad_norm": 1.1715062856674194,
      "learning_rate": 4.002857142857143e-06,
      "loss": 0.4813,
      "step": 2600
    },
    {
      "epoch": 11.345218800648299,
      "grad_norm": 1.149922490119934,
      "learning_rate": 3.931428571428571e-06,
      "loss": 0.4809,
      "step": 2625
    },
    {
      "epoch": 11.453268503511616,
      "grad_norm": 1.204345941543579,
      "learning_rate": 3.86e-06,
      "loss": 0.4815,
      "step": 2650
    },
    {
      "epoch": 11.561318206374933,
      "grad_norm": 1.0901967287063599,
      "learning_rate": 3.7885714285714285e-06,
      "loss": 0.4834,
      "step": 2675
    },
    {
      "epoch": 11.66936790923825,
      "grad_norm": 1.1404497623443604,
      "learning_rate": 3.7171428571428575e-06,
      "loss": 0.4774,
      "step": 2700
    },
    {
      "epoch": 11.777417612101567,
      "grad_norm": 1.155362844467163,
      "learning_rate": 3.6457142857142857e-06,
      "loss": 0.4806,
      "step": 2725
    },
    {
      "epoch": 11.885467314964885,
      "grad_norm": 1.4337153434753418,
      "learning_rate": 3.5742857142857147e-06,
      "loss": 0.4834,
      "step": 2750
    },
    {
      "epoch": 11.993517017828202,
      "grad_norm": 1.321986198425293,
      "learning_rate": 3.5028571428571433e-06,
      "loss": 0.4846,
      "step": 2775
    },
    {
      "epoch": 12.101566720691519,
      "grad_norm": 1.251910924911499,
      "learning_rate": 3.431428571428572e-06,
      "loss": 0.4757,
      "step": 2800
    },
    {
      "epoch": 12.209616423554834,
      "grad_norm": 1.5648906230926514,
      "learning_rate": 3.3600000000000004e-06,
      "loss": 0.4785,
      "step": 2825
    },
    {
      "epoch": 12.317666126418152,
      "grad_norm": 1.5434080362319946,
      "learning_rate": 3.2885714285714286e-06,
      "loss": 0.4826,
      "step": 2850
    },
    {
      "epoch": 12.425715829281469,
      "grad_norm": 1.2286030054092407,
      "learning_rate": 3.2171428571428576e-06,
      "loss": 0.485,
      "step": 2875
    },
    {
      "epoch": 12.533765532144786,
      "grad_norm": 1.3100131750106812,
      "learning_rate": 3.1457142857142858e-06,
      "loss": 0.4823,
      "step": 2900
    },
    {
      "epoch": 12.641815235008103,
      "grad_norm": 1.0240205526351929,
      "learning_rate": 3.074285714285715e-06,
      "loss": 0.4834,
      "step": 2925
    },
    {
      "epoch": 12.74986493787142,
      "grad_norm": 0.9350565075874329,
      "learning_rate": 3.002857142857143e-06,
      "loss": 0.4815,
      "step": 2950
    },
    {
      "epoch": 12.857914640734737,
      "grad_norm": 1.4391809701919556,
      "learning_rate": 2.9314285714285716e-06,
      "loss": 0.4805,
      "step": 2975
    },
    {
      "epoch": 12.965964343598054,
      "grad_norm": 0.9421512484550476,
      "learning_rate": 2.86e-06,
      "loss": 0.48,
      "step": 3000
    },
    {
      "epoch": 12.965964343598054,
      "eval_loss": 0.44735169410705566,
      "eval_runtime": 28.548,
      "eval_samples_per_second": 28.829,
      "eval_steps_per_second": 14.432,
      "step": 3000
    },
    {
      "epoch": 13.074014046461372,
      "grad_norm": 1.5335721969604492,
      "learning_rate": 2.7885714285714287e-06,
      "loss": 0.4864,
      "step": 3025
    },
    {
      "epoch": 13.182063749324689,
      "grad_norm": 1.1898924112319946,
      "learning_rate": 2.7171428571428577e-06,
      "loss": 0.4846,
      "step": 3050
    },
    {
      "epoch": 13.290113452188006,
      "grad_norm": 1.1927522420883179,
      "learning_rate": 2.645714285714286e-06,
      "loss": 0.4847,
      "step": 3075
    },
    {
      "epoch": 13.398163155051323,
      "grad_norm": 1.2950398921966553,
      "learning_rate": 2.574285714285715e-06,
      "loss": 0.4798,
      "step": 3100
    },
    {
      "epoch": 13.50621285791464,
      "grad_norm": 1.7267613410949707,
      "learning_rate": 2.502857142857143e-06,
      "loss": 0.484,
      "step": 3125
    },
    {
      "epoch": 13.614262560777957,
      "grad_norm": 1.2066303491592407,
      "learning_rate": 2.4314285714285717e-06,
      "loss": 0.4826,
      "step": 3150
    },
    {
      "epoch": 13.722312263641275,
      "grad_norm": 2.8694841861724854,
      "learning_rate": 2.3600000000000003e-06,
      "loss": 0.4734,
      "step": 3175
    },
    {
      "epoch": 13.830361966504592,
      "grad_norm": 0.9672777056694031,
      "learning_rate": 2.288571428571429e-06,
      "loss": 0.4752,
      "step": 3200
    },
    {
      "epoch": 13.938411669367909,
      "grad_norm": 1.0579876899719238,
      "learning_rate": 2.2171428571428575e-06,
      "loss": 0.4779,
      "step": 3225
    },
    {
      "epoch": 14.046461372231226,
      "grad_norm": 1.3300089836120605,
      "learning_rate": 2.145714285714286e-06,
      "loss": 0.4827,
      "step": 3250
    },
    {
      "epoch": 14.154511075094543,
      "grad_norm": 1.2969493865966797,
      "learning_rate": 2.0742857142857146e-06,
      "loss": 0.4742,
      "step": 3275
    },
    {
      "epoch": 14.26256077795786,
      "grad_norm": 1.4201631546020508,
      "learning_rate": 2.0028571428571432e-06,
      "loss": 0.4812,
      "step": 3300
    },
    {
      "epoch": 14.370610480821178,
      "grad_norm": 1.0666288137435913,
      "learning_rate": 1.9314285714285714e-06,
      "loss": 0.4802,
      "step": 3325
    },
    {
      "epoch": 14.478660183684495,
      "grad_norm": 1.007049322128296,
      "learning_rate": 1.8600000000000002e-06,
      "loss": 0.4811,
      "step": 3350
    },
    {
      "epoch": 14.586709886547812,
      "grad_norm": 1.0587021112442017,
      "learning_rate": 1.7885714285714288e-06,
      "loss": 0.475,
      "step": 3375
    },
    {
      "epoch": 14.69475958941113,
      "grad_norm": 0.9532779455184937,
      "learning_rate": 1.7171428571428572e-06,
      "loss": 0.4784,
      "step": 3400
    },
    {
      "epoch": 14.802809292274446,
      "grad_norm": 1.2507985830307007,
      "learning_rate": 1.6457142857142857e-06,
      "loss": 0.4754,
      "step": 3425
    },
    {
      "epoch": 14.910858995137763,
      "grad_norm": 0.941355288028717,
      "learning_rate": 1.5742857142857143e-06,
      "loss": 0.4802,
      "step": 3450
    },
    {
      "epoch": 15.01890869800108,
      "grad_norm": 1.2042776346206665,
      "learning_rate": 1.502857142857143e-06,
      "loss": 0.4776,
      "step": 3475
    },
    {
      "epoch": 15.126958400864398,
      "grad_norm": 1.0119942426681519,
      "learning_rate": 1.4314285714285717e-06,
      "loss": 0.4777,
      "step": 3500
    },
    {
      "epoch": 15.235008103727715,
      "grad_norm": 1.1111578941345215,
      "learning_rate": 1.3600000000000001e-06,
      "loss": 0.4794,
      "step": 3525
    },
    {
      "epoch": 15.343057806591032,
      "grad_norm": 1.2696534395217896,
      "learning_rate": 1.2885714285714287e-06,
      "loss": 0.4767,
      "step": 3550
    },
    {
      "epoch": 15.45110750945435,
      "grad_norm": 0.9985325336456299,
      "learning_rate": 1.2171428571428573e-06,
      "loss": 0.4745,
      "step": 3575
    },
    {
      "epoch": 15.559157212317666,
      "grad_norm": 1.167740821838379,
      "learning_rate": 1.1457142857142859e-06,
      "loss": 0.4804,
      "step": 3600
    },
    {
      "epoch": 15.667206915180984,
      "grad_norm": 1.3728479146957397,
      "learning_rate": 1.0742857142857145e-06,
      "loss": 0.4765,
      "step": 3625
    },
    {
      "epoch": 15.7752566180443,
      "grad_norm": 1.180932641029358,
      "learning_rate": 1.0028571428571428e-06,
      "loss": 0.4778,
      "step": 3650
    },
    {
      "epoch": 15.883306320907618,
      "grad_norm": 1.0790457725524902,
      "learning_rate": 9.314285714285714e-07,
      "loss": 0.4768,
      "step": 3675
    },
    {
      "epoch": 15.991356023770935,
      "grad_norm": 1.037518858909607,
      "learning_rate": 8.6e-07,
      "loss": 0.477,
      "step": 3700
    },
    {
      "epoch": 16.099405726634252,
      "grad_norm": 1.4232577085494995,
      "learning_rate": 7.885714285714287e-07,
      "loss": 0.4777,
      "step": 3725
    },
    {
      "epoch": 16.207455429497568,
      "grad_norm": 1.1463338136672974,
      "learning_rate": 7.171428571428572e-07,
      "loss": 0.4778,
      "step": 3750
    },
    {
      "epoch": 16.315505132360887,
      "grad_norm": 1.4168473482131958,
      "learning_rate": 6.457142857142858e-07,
      "loss": 0.4808,
      "step": 3775
    },
    {
      "epoch": 16.423554835224202,
      "grad_norm": 1.110643744468689,
      "learning_rate": 5.742857142857143e-07,
      "loss": 0.4799,
      "step": 3800
    },
    {
      "epoch": 16.53160453808752,
      "grad_norm": 1.0503230094909668,
      "learning_rate": 5.028571428571429e-07,
      "loss": 0.4774,
      "step": 3825
    },
    {
      "epoch": 16.639654240950836,
      "grad_norm": 1.260421633720398,
      "learning_rate": 4.3142857142857146e-07,
      "loss": 0.4838,
      "step": 3850
    },
    {
      "epoch": 16.747703943814155,
      "grad_norm": 1.289737343788147,
      "learning_rate": 3.6e-07,
      "loss": 0.4766,
      "step": 3875
    },
    {
      "epoch": 16.85575364667747,
      "grad_norm": 0.9437748789787292,
      "learning_rate": 2.885714285714286e-07,
      "loss": 0.4741,
      "step": 3900
    },
    {
      "epoch": 16.96380334954079,
      "grad_norm": 0.977416455745697,
      "learning_rate": 2.1714285714285715e-07,
      "loss": 0.4757,
      "step": 3925
    },
    {
      "epoch": 17.071853052404105,
      "grad_norm": 1.014186143875122,
      "learning_rate": 1.4571428571428574e-07,
      "loss": 0.4761,
      "step": 3950
    },
    {
      "epoch": 17.179902755267424,
      "grad_norm": 1.2756359577178955,
      "learning_rate": 7.428571428571429e-08,
      "loss": 0.4755,
      "step": 3975
    },
    {
      "epoch": 17.28795245813074,
      "grad_norm": 1.0961743593215942,
      "learning_rate": 2.8571428571428576e-09,
      "loss": 0.4814,
      "step": 4000
    },
    {
      "epoch": 17.28795245813074,
      "eval_loss": 0.4458176791667938,
      "eval_runtime": 30.9471,
      "eval_samples_per_second": 26.594,
      "eval_steps_per_second": 13.313,
      "step": 4000
    }
  ],
  "logging_steps": 25,
  "max_steps": 4000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 18,
  "save_steps": 1000,
  "total_flos": 1.8395787548977344e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
