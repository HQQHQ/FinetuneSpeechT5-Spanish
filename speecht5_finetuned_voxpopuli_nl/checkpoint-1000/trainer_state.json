{
  "best_metric": 0.46402496099472046,
  "best_model_checkpoint": "speecht5_finetuned_voxpopuli_nl/checkpoint-1000",
  "epoch": 4.321988114532685,
  "eval_steps": 1000,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.10804970286331712,
      "grad_norm": 4.890783309936523,
      "learning_rate": 5.000000000000001e-07,
      "loss": 0.7958,
      "step": 25
    },
    {
      "epoch": 0.21609940572663425,
      "grad_norm": 2.898012638092041,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 0.7689,
      "step": 50
    },
    {
      "epoch": 0.3241491085899514,
      "grad_norm": 3.471942901611328,
      "learning_rate": 1.48e-06,
      "loss": 0.7529,
      "step": 75
    },
    {
      "epoch": 0.4321988114532685,
      "grad_norm": 2.7499890327453613,
      "learning_rate": 1.98e-06,
      "loss": 0.7401,
      "step": 100
    },
    {
      "epoch": 0.5402485143165856,
      "grad_norm": 2.962912082672119,
      "learning_rate": 2.4800000000000004e-06,
      "loss": 0.7048,
      "step": 125
    },
    {
      "epoch": 0.6482982171799028,
      "grad_norm": 2.0028252601623535,
      "learning_rate": 2.9800000000000003e-06,
      "loss": 0.6826,
      "step": 150
    },
    {
      "epoch": 0.7563479200432199,
      "grad_norm": 2.515033721923828,
      "learning_rate": 3.48e-06,
      "loss": 0.6655,
      "step": 175
    },
    {
      "epoch": 0.864397622906537,
      "grad_norm": 2.316354751586914,
      "learning_rate": 3.980000000000001e-06,
      "loss": 0.6526,
      "step": 200
    },
    {
      "epoch": 0.9724473257698542,
      "grad_norm": 1.84903883934021,
      "learning_rate": 4.48e-06,
      "loss": 0.6532,
      "step": 225
    },
    {
      "epoch": 1.0804970286331712,
      "grad_norm": 1.8633564710617065,
      "learning_rate": 4.980000000000001e-06,
      "loss": 0.6393,
      "step": 250
    },
    {
      "epoch": 1.1885467314964884,
      "grad_norm": 1.7015737295150757,
      "learning_rate": 5.480000000000001e-06,
      "loss": 0.6304,
      "step": 275
    },
    {
      "epoch": 1.2965964343598055,
      "grad_norm": 2.144501209259033,
      "learning_rate": 5.98e-06,
      "loss": 0.6049,
      "step": 300
    },
    {
      "epoch": 1.4046461372231227,
      "grad_norm": 2.430676221847534,
      "learning_rate": 6.480000000000001e-06,
      "loss": 0.5994,
      "step": 325
    },
    {
      "epoch": 1.5126958400864399,
      "grad_norm": 1.6690797805786133,
      "learning_rate": 6.98e-06,
      "loss": 0.5709,
      "step": 350
    },
    {
      "epoch": 1.620745542949757,
      "grad_norm": 1.7207589149475098,
      "learning_rate": 7.48e-06,
      "loss": 0.5645,
      "step": 375
    },
    {
      "epoch": 1.728795245813074,
      "grad_norm": 2.163778781890869,
      "learning_rate": 7.980000000000002e-06,
      "loss": 0.5607,
      "step": 400
    },
    {
      "epoch": 1.8368449486763911,
      "grad_norm": 1.4455478191375732,
      "learning_rate": 8.48e-06,
      "loss": 0.5574,
      "step": 425
    },
    {
      "epoch": 1.9448946515397083,
      "grad_norm": 1.6066086292266846,
      "learning_rate": 8.98e-06,
      "loss": 0.5436,
      "step": 450
    },
    {
      "epoch": 2.0529443544030253,
      "grad_norm": 1.5660029649734497,
      "learning_rate": 9.48e-06,
      "loss": 0.5466,
      "step": 475
    },
    {
      "epoch": 2.1609940572663424,
      "grad_norm": 1.3218941688537598,
      "learning_rate": 9.980000000000001e-06,
      "loss": 0.5382,
      "step": 500
    },
    {
      "epoch": 2.2690437601296596,
      "grad_norm": 1.5174471139907837,
      "learning_rate": 9.931428571428571e-06,
      "loss": 0.5419,
      "step": 525
    },
    {
      "epoch": 2.3770934629929767,
      "grad_norm": 1.1267651319503784,
      "learning_rate": 9.86e-06,
      "loss": 0.5254,
      "step": 550
    },
    {
      "epoch": 2.485143165856294,
      "grad_norm": 1.5505274534225464,
      "learning_rate": 9.78857142857143e-06,
      "loss": 0.5316,
      "step": 575
    },
    {
      "epoch": 2.593192868719611,
      "grad_norm": 1.5775372982025146,
      "learning_rate": 9.717142857142858e-06,
      "loss": 0.5252,
      "step": 600
    },
    {
      "epoch": 2.7012425715829282,
      "grad_norm": 1.435638189315796,
      "learning_rate": 9.645714285714286e-06,
      "loss": 0.5261,
      "step": 625
    },
    {
      "epoch": 2.8092922744462454,
      "grad_norm": 1.428922414779663,
      "learning_rate": 9.574285714285715e-06,
      "loss": 0.5276,
      "step": 650
    },
    {
      "epoch": 2.9173419773095626,
      "grad_norm": 1.7638425827026367,
      "learning_rate": 9.502857142857144e-06,
      "loss": 0.524,
      "step": 675
    },
    {
      "epoch": 3.0253916801728797,
      "grad_norm": 1.120676875114441,
      "learning_rate": 9.431428571428573e-06,
      "loss": 0.5258,
      "step": 700
    },
    {
      "epoch": 3.1334413830361965,
      "grad_norm": 1.4151748418807983,
      "learning_rate": 9.360000000000002e-06,
      "loss": 0.515,
      "step": 725
    },
    {
      "epoch": 3.2414910858995136,
      "grad_norm": 1.3693028688430786,
      "learning_rate": 9.28857142857143e-06,
      "loss": 0.517,
      "step": 750
    },
    {
      "epoch": 3.349540788762831,
      "grad_norm": 1.541468620300293,
      "learning_rate": 9.217142857142858e-06,
      "loss": 0.5197,
      "step": 775
    },
    {
      "epoch": 3.457590491626148,
      "grad_norm": 1.2785146236419678,
      "learning_rate": 9.145714285714287e-06,
      "loss": 0.5117,
      "step": 800
    },
    {
      "epoch": 3.565640194489465,
      "grad_norm": 1.4224395751953125,
      "learning_rate": 9.074285714285716e-06,
      "loss": 0.5104,
      "step": 825
    },
    {
      "epoch": 3.6736898973527823,
      "grad_norm": 1.2443407773971558,
      "learning_rate": 9.002857142857144e-06,
      "loss": 0.5087,
      "step": 850
    },
    {
      "epoch": 3.7817396002160995,
      "grad_norm": 1.39913010597229,
      "learning_rate": 8.931428571428573e-06,
      "loss": 0.5118,
      "step": 875
    },
    {
      "epoch": 3.8897893030794166,
      "grad_norm": 1.1192206144332886,
      "learning_rate": 8.860000000000002e-06,
      "loss": 0.5069,
      "step": 900
    },
    {
      "epoch": 3.997839005942734,
      "grad_norm": 1.358802318572998,
      "learning_rate": 8.788571428571429e-06,
      "loss": 0.5118,
      "step": 925
    },
    {
      "epoch": 4.1058887088060505,
      "grad_norm": 1.4780887365341187,
      "learning_rate": 8.717142857142858e-06,
      "loss": 0.503,
      "step": 950
    },
    {
      "epoch": 4.213938411669368,
      "grad_norm": 1.0592700242996216,
      "learning_rate": 8.645714285714287e-06,
      "loss": 0.5066,
      "step": 975
    },
    {
      "epoch": 4.321988114532685,
      "grad_norm": 1.2728278636932373,
      "learning_rate": 8.574285714285714e-06,
      "loss": 0.506,
      "step": 1000
    },
    {
      "epoch": 4.321988114532685,
      "eval_loss": 0.46402496099472046,
      "eval_runtime": 31.9452,
      "eval_samples_per_second": 25.763,
      "eval_steps_per_second": 12.897,
      "step": 1000
    }
  ],
  "logging_steps": 25,
  "max_steps": 4000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 18,
  "save_steps": 1000,
  "total_flos": 4596277585610304.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
